from pyspark.sql import SparkSession
import getpass
username = getpass.getuser()
spark = SparkSession. \
builder. \
config('spark.ui.port','0'). \
config("spark.sql.warehouse.dir", f"/user/itv000173/warehouse"). \
enableHiveSupport(). \
master('yarn'). \
getOrCreate()

spark

words = ["BIG","DATA","IS","SUPER","INTERESTING","BIG","DATA","IS","A","TRENDING","TECHNOLOGY"]
words_rdd = spark.sparkContext.parallelize(words)
words_normalized = words_rdd.map(lambda x:x.lower())
words_rdd.getNumPartitions()
spark.sparkContext.defaultParallelism
orders_rdd = spark.sparkContext.textFile("/public/trendytech/retail_db/orders/*")
mapped_rdd = orders_rdd.map(lambda x: (x.split(",")[3],1))
reduced_rdd = mapped_rdd.reduceByKey(lambda x,y :x+y)
reduced_rdd.take(10)
orders_rdd = spark.sparkContext.textFile("/public/trendytech/retail_db/orders/*")
mapped_rdd = orders_rdd.map(lambda x: (x.split(",")[3]))
mapped_rdd.collect()
mapped_rdd.countByValue()

reduceByKey   VS countBYkey

reduceBykey is a transformation when you call reducebykey you are not getting the same result you have to say collect.

Instead of map+reducebykey we should use  countbyvalue
